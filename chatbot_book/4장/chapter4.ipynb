{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 임베딩(단어 수치화 전처리)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one-hot -encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '날씨', '구름']\n",
      "{'오늘': 0, '날씨': 1, '구름': 2}\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#원-핫 인코딩\n",
    "from konlpy.tag import Komoran\n",
    "import numpy as np\n",
    "\n",
    "komoran = Komoran()\n",
    "text = '오늘 날씨는 구름이 많아요'\n",
    "\n",
    "#명사만 추출\n",
    "nouns = komoran.nouns(text)\n",
    "print(nouns)\n",
    "\n",
    "#단어 사전 구축 및 단어별 인덱스 부여\n",
    "dics ={}\n",
    "for word in nouns:\n",
    "    if word not in dics.keys():\n",
    "        dics[word] = len(dics)\n",
    "print(dics)   \n",
    "\n",
    "# 원-핫 인코딩\n",
    "nb_classes = len(dics) #원-핫 벡터 차원의 크기를 결정, 단어 사전의 크기가 원핫 벡터의 크기\n",
    "targets = list(dics.values()) #원핫 인코딩 기능을 사용하기 위해 딕셔너리의 value값을 list로 변환\n",
    "one_hot_targets = np.eye(nb_classes)[targets]\n",
    "# eye함수는 단위행렬을 만들어줌 eye함수 뒤에 붙은 [target]를 이용해 생성된 단위행렬의 순서를 단어 사전의 순서에 맞게 정렬해줌\n",
    "print(one_hot_targets)\n",
    "\n",
    "#오늘: 1 0 0\n",
    "#날씨: 0 1 0\n",
    "#구름: 0 0 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#에제로 네이버 영화 리뷰 말뭉치 데이터 사용\n",
    "#자연어 처리 위한 라이브러리 Gensim패키지 사용\n",
    "from gensim.models import Word2Vec\n",
    "from konlpy.tag import Komoran\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 영화 리뷰 데이터 읽어오기\n",
    "#한글파일(웹페이지)는 왠만하면 utf-8 아니면 euc-kr로 인코딩 되어있다!\n",
    "def read_review_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[1:] #헤더 제거\n",
    "    return data\n",
    "\n",
    "#들어온 데이터가 /t를 기준으로 데이터를 분리함\n",
    "#그 후 첫 번째 행의 헤더를 제거하고 리뷰 데이터만 반환함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "말뭉치 데이터 읽기 시작\n",
      "200000\n",
      "말뭉치 데이터 읽기 완료:  1.8244166374206543\n"
     ]
    }
   ],
   "source": [
    "#학습 시간 측정 시작\n",
    "start = time.time()\n",
    "\n",
    "#리뷰 파일 읽어오기\n",
    "print('말뭉치 데이터 읽기 시작')\n",
    "review_data = read_review_data('./nsmc-master/ratings.txt') \n",
    "# read_review_data 함수를 호출해 ratings.txt 파일을 리스트 형태로 읽어옴\n",
    "print(len(review_data)) # 리뷰 데이터 전체 개수\n",
    "print('말뭉치 데이터 읽기 완료: ', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소에서 명사만 추출 시작\n",
      "형태소에서 명사만 추출완료:  287.51341009140015\n"
     ]
    }
   ],
   "source": [
    "# 문장 단위로 명사만 추출해 학습 입력 데이터로 만듦\n",
    "print('형태소에서 명사만 추출 시작')\n",
    "komoran = Komoran()\n",
    "docs = [komoran.nouns(sentence[1]) for sentence in review_data] \n",
    "# sentence[1]은 rating.txt파일에서 document 칼럼의 데이터를 의미함\n",
    "print('형태소에서 명사만 추출완료: ', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec 모델 학습 시작\n",
      "word2vec 모델 학습 완료:  361.64774775505066\n"
     ]
    }
   ],
   "source": [
    "# word2vec 모델 학습\n",
    "print('word2vec 모델 학습 시작')\n",
    "model = Word2Vec(sentences=docs, vector_size=200, window=4, hs=1, min_count=2, sg=1)\n",
    "print('word2vec 모델 학습 완료: ', time.time() - start)\n",
    "#명사만 추출한 곳에서 word2vec 모델을 학습시킴 \n",
    "#주요 하이퍼파라미터\n",
    "# sentences: word2vec 모델 학습에 필요한 문장 데이터. word2vec 모델의 입력값으로 사용됨\n",
    "# size: 단어 임베딩 벡터의 차원(크기)\n",
    "# winodw: 주변 단어 윈도우의 크기\n",
    "# hs: 0(0이 아닌 경우 음수 샘플링 사용), 1 (모델 학습에 softmax사용)\n",
    "# min_count: 단어 최소 빈도 수 제한(설정된 빈도 수 이하의 단어들은 학습하지 않음)\n",
    "#sg: 0(cbow 모델) 1(skip-gram 모델)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 저장 시작\n",
      "학습된 모델 저장 완료:  400.1078898906708\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장\n",
    "print('모델 저장 시작')\n",
    "model.save('nvmc.model')\n",
    "print('학습된 모델 저장 완료: ',time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_count: 200000\n",
      "corpus_total_words:  1076896\n"
     ]
    }
   ],
   "source": [
    "#학습된 말뭉치 수, 코퍼스 내 전체 단어 수\n",
    "print('corpus_count:', model.corpus_count)\n",
    "print('corpus_total_words: ', model.corpus_total_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위에서 한 예제 유사도 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coupu_total_words:  1076896\n",
      "사랑:  [-4.15605679e-02 -3.80127549e-01  1.61127716e-01  5.71583118e-03\n",
      " -5.89701124e-02 -3.86713356e-01 -4.48233373e-02  1.22257210e-01\n",
      "  1.86522473e-02 -1.58316374e-01 -3.80085111e-02  4.05164100e-02\n",
      "  1.96985215e-01  8.85017291e-02 -2.34512106e-01  3.29363614e-01\n",
      " -3.15285288e-03 -2.53304821e-02  1.37879848e-02 -4.56013292e-01\n",
      "  1.90241456e-01  3.94645557e-02 -2.20273763e-01  3.70881647e-01\n",
      " -3.90942931e-01 -1.29743010e-01  1.65335774e-01 -4.33890760e-01\n",
      " -1.17295861e-01 -1.13350980e-01  3.92541029e-02  2.02908397e-01\n",
      "  7.84973148e-03 -2.42442951e-01  1.64653763e-01  4.73551959e-01\n",
      "  1.20241977e-01 -1.18682496e-01 -2.67837882e-01  9.67743471e-02\n",
      " -4.64657135e-03  1.07407257e-01 -9.09824073e-02 -2.58279264e-01\n",
      "  4.84912902e-01  3.10303032e-01 -3.22651267e-01  1.51932975e-02\n",
      "  2.50542164e-01 -2.45144013e-02  1.88086197e-01 -2.68738002e-01\n",
      " -1.06334642e-01 -3.18943150e-02  1.01010732e-01  2.60324031e-01\n",
      "  2.89106935e-01 -2.40909040e-01  1.76846668e-01 -2.42138833e-01\n",
      " -3.85332257e-02  7.46469274e-02 -5.96209168e-02  4.11491059e-02\n",
      "  1.61719143e-01 -1.64006740e-01 -4.85728353e-01 -7.51574263e-02\n",
      "  1.29724769e-02  6.26851618e-01 -1.13515668e-01 -4.37661827e-01\n",
      "  4.46031988e-02 -1.02851070e-01  1.21663049e-01  2.70975083e-02\n",
      "  1.64053932e-01 -1.01958178e-01 -3.08082581e-01 -1.26494199e-01\n",
      " -2.18992636e-01 -1.76406167e-02  1.35053158e-01  3.62677932e-01\n",
      " -2.67245203e-01 -2.73228347e-01  3.04642003e-02  1.61529271e-04\n",
      " -1.42052457e-01 -2.07474619e-01  6.11989126e-02  1.41774580e-01\n",
      "  1.52615681e-01 -1.81998044e-01  3.75567943e-01 -1.11941144e-01\n",
      " -1.08091846e-01 -2.76493490e-01  3.84580567e-02 -2.67279092e-02\n",
      " -3.80359381e-01  2.87081033e-01 -9.61553007e-02  1.49500996e-01\n",
      "  1.40822418e-02 -7.72147104e-02 -8.11447278e-02  2.29410022e-01\n",
      " -1.57055259e-02 -4.11136866e-01  9.32550505e-02 -4.09779608e-01\n",
      " -4.24353480e-02  2.52344638e-01 -1.43467769e-01 -2.38979724e-03\n",
      " -4.29015309e-02  7.78877735e-02  1.00560851e-01 -2.05284342e-01\n",
      " -3.56777629e-04 -1.04348607e-01 -2.53219903e-01 -1.22382991e-01\n",
      "  8.42176676e-02  1.92320213e-01 -3.79560553e-02  2.78764635e-01\n",
      "  1.26569614e-01 -1.17038108e-01 -4.17915396e-02  4.64883558e-02\n",
      " -1.52175501e-01 -7.06515461e-02  1.15033537e-01  2.04981685e-01\n",
      "  3.13451499e-01 -1.08848713e-01 -5.46595529e-02  1.12587437e-01\n",
      " -3.75669748e-02 -3.88038725e-01 -1.22743950e-04 -1.04875326e-01\n",
      "  1.65218309e-01 -2.48458803e-01 -1.10263586e-01 -1.18874460e-01\n",
      " -1.29207168e-02  2.50680521e-02  3.28670628e-02 -1.41371274e-02\n",
      "  1.63408652e-01  1.50305226e-01 -2.12806180e-01  8.09133053e-02\n",
      "  8.90954956e-02 -1.16506964e-01  1.06598280e-01 -3.62718433e-01\n",
      "  7.15182200e-02 -1.18158862e-01  4.71756049e-02 -1.49266139e-01\n",
      " -2.38915265e-01  1.72875956e-01 -1.67762991e-02 -1.18459202e-01\n",
      " -1.33547023e-01  6.06233105e-02 -3.55552554e-01  2.98088849e-01\n",
      "  4.45946336e-01 -1.48888513e-01  8.08281526e-02  3.20333615e-02\n",
      "  1.88054442e-01 -2.05772445e-01  3.12475324e-01 -1.34904668e-01\n",
      "  7.99123012e-03 -4.16304404e-03  9.84670520e-02 -3.90260033e-02\n",
      "  1.93558231e-01  1.21498737e-03 -3.53269055e-02 -7.05867112e-02\n",
      "  3.44485819e-01  1.67206019e-01  2.37615660e-01 -5.19393384e-01\n",
      "  1.52465319e-02  5.11132032e-02 -1.02905128e-02 -1.75026700e-01\n",
      " -2.54674584e-01 -1.05388544e-01  2.52324939e-01 -5.78261837e-02]\n",
      "일요일 = 월요일\t 0.6664964\n"
     ]
    }
   ],
   "source": [
    "#위에서 생성된 Word2Vec 모델 파일을 읽어와 실제 단어 임베딩된 값과 벡터 공간상의 유사한 단어들 확인 예제\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#모델 로딩\n",
    "model = Word2Vec.load('nvmc.model')\n",
    "print('coupu_total_words: ', model.corpus_total_words)\n",
    "\n",
    "#'사랑'이란 단어로 생성한 단어 임베딩 벡터\n",
    "print('사랑: ',model.wv['사랑'])\n",
    "# 사랑 이라는 단어로 생성한 단어 임베딩 벡터이다.\n",
    "#모델을 학습할 때 설정한 size 하이퍼파라미터만큼 단어 임베딩 벡터 차원 크기가 결정됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "일요일 = 월요일\t 0.6664964\n",
      "안성기 = 배우\t 0.56402194\n",
      "대기업 = 삼성\t 0.54993635\n",
      "일요일 = 삼성\t 0.2801627\n",
      "히어로 = 월요일\t 0.04280297\n"
     ]
    }
   ],
   "source": [
    "#단어 유사도 계산\n",
    "print('일요일 = 월요일\\t', model.wv.similarity(w1='일요일', w2='월요일'))\n",
    "print('안성기 = 배우\\t', model.wv.similarity(w1='안성기', w2='배우'))\n",
    "print('대기업 = 삼성\\t', model.wv.similarity(w1='대기업', w2='삼성'))\n",
    "print('일요일 = 삼성\\t', model.wv.similarity(w1='일요일', w2='삼성'))\n",
    "print('히어로 = 월요일\\t', model.wv.similarity(w1='히어로', w2='월요일'))\n",
    "\n",
    "#gensim 패키지의 model.wv.most_similar() 함수를 호출할 경우 인자로 사용한 단어와 가장 유사한 단어를 리스트로 반환해줌\n",
    "# 즉 벡터 공간에서 가장 가까운 거리에 있는 단어들을 반환함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('씨야', 0.7382012009620667), ('이은우', 0.7302901744842529), ('능청', 0.7158526182174683), ('김수로', 0.7059884071350098), ('김갑수', 0.7055827975273132)]\n",
      "[('더 울버린', 0.672785758972168), ('X맨', 0.6529032588005066), ('나니아 연대기', 0.6478983163833618), ('기사단', 0.647780179977417), ('파라노말 액티비티', 0.6380230188369751)]\n"
     ]
    }
   ],
   "source": [
    "#가장 유사한 단어 추출\n",
    "print(model.wv.most_similar('안성기', topn=5)) #topn 인자는 반환되는 유사한 단어 수를 의미하며 topn=5는 5개까지 유사한 단어를 반환했다.\n",
    "print(model.wv.most_similar('시리즈', topn=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3278a805e5321cd8f38b6107a6ed0dbe03bcc4e7f61f4414bddfd4b241e95b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
