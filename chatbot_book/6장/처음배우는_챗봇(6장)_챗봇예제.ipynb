{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHGvmAHMxUXP"
      },
      "source": [
        "# 6.2.2 챗본 문답 데이터 감정 분류 모델 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AbBbfAlwaTF",
        "outputId": "ec1fed4e-0959-4504-e393-b434be17a1cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Chatbot_data'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 60 (delta 23), reused 5 (delta 3), pack-reused 18\u001b[K\n",
            "Unpacking objects: 100% (60/60), 463.73 KiB | 4.64 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/songys/Chatbot_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ME7rwAuNwi7P"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lrvFaCoxRJx"
      },
      "source": [
        "### 데이터 읽어오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "73YKk1uTxPRX"
      },
      "outputs": [],
      "source": [
        "train_file = '/content/Chatbot_data/ChatbotData.csv'\n",
        "data = pd.read_csv(train_file, delimiter=',')\n",
        "features = data['Q'].tolist()\n",
        "labels = data['label'].tolist()\n",
        "\n",
        "#chatbotdata 파일을 읽어와 CNN모델 학습시 필요한 Q(질문)와 label(감정) 데이터를 features와 labels 리스트에 저장함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "svj60m7AxrIO"
      },
      "outputs": [],
      "source": [
        "# 단어 인덱스 시퀀스 벡터 2\n",
        "\n",
        "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# 1에서 불러온 질문 리스트(features)에서 문장을 하나씩 꺼내와 text_to_word_sequence()함수를 이용해 단어 시퀀스를 만듦\n",
        "# 단어 시퀀ㅅ스란 토큰들의 순차적 리스트를 의미함\n",
        "# 예를 들어 '3박4일 놀러가고 싶다'면 ['3박4일', '놀러가고', '싶다']이렇게 된다.\n",
        "# 위 처럼 생성된 단어 시퀀스를 말뭉치(corpus) 리스트에 저장함\n",
        "# 다음 texts_to_sequences()함수를 이용해 문장 내 모든 단어를 시퀀스 번호(벡터)로 변환함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3KUO6Wdmym8O"
      },
      "outputs": [],
      "source": [
        "MAX_SEQ_LEN = 15 #단어 시퀀스 벡터 크기\n",
        "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
        "\n",
        "#벡터 크기가 재각각이여서 학습시키려면 같게 만들어줘야함\n",
        "# max_seq_len을 이용해 벡터 크기 맞춤\n",
        "# 크기가 작은 단어는 공간이 남는데 그 부분을 0으로 채워준다.\n",
        "# 이 과정을 패딩 padding이라 한다. \n",
        "# 케라스에서 pad_sequences() 함수를 통해 시퀀스의 패딩 처리를 손쉽게 할 수 있다.\n",
        "# maxlen 인자로 시퀀스의 최대 길이를 정하는데 학습시킬 문장 데이터들을 사전에 분석해 파악해야한다.\n",
        "# 너무 크면 자원 낭비, 작게 잡으면 데이터 손실이 발생"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PmWol01dzCbv"
      },
      "outputs": [],
      "source": [
        "#학습용, 검증용, 데이터셋 생성 3\n",
        "#학습셋:검증셋:데이트셋 7:2:1\n",
        "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
        "ds = ds.shuffle(len(features))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kNmOxQJwzOts"
      },
      "outputs": [],
      "source": [
        "train_size = int(len(padded_seqs) * 0.7)\n",
        "val_size = int(len(padded_seqs) * 0.2)\n",
        "test_size = int(len(padded_seqs) * 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zSQsua_Wzw0M"
      },
      "outputs": [],
      "source": [
        "train_ds = ds.take(train_size).batch(20)\n",
        "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
        "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "clIInSxW0ALb"
      },
      "outputs": [],
      "source": [
        "#하이퍼파라미터 설정\n",
        "dropout_prob = 0.5\n",
        "EMB_SIZE =128\n",
        "epoch =5\n",
        "vocab_size =len(word_index) + 1 #전체 단어 수 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7MVzJoxf2MCt"
      },
      "outputs": [],
      "source": [
        "#CNN 모델 정의 4\n",
        "#단어 임베딩 처리 영역\n",
        "input_layer = Input(shape = (MAX_SEQ_LEN,))\n",
        "embedding_layer = Embedding(vocab_size, EMB_SIZE, input_length = MAX_SEQ_LEN)(input_layer)\n",
        "# vocab_size = 단어 개수, EMB_SIZE= 임베딩 결과로 나올 밀집 벡터의 크기, input_length= 입력되는 시퀀스 벡터의 크기를 embedding()의 인자로 사용해 임베딩 계층을 생성\n",
        "dropout_emb = Dropout(rate = dropout_prob)(embedding_layer) #dropout=0.5\n",
        "\n",
        "#합성곱 필터와 연산을 통해 문장의 특징 정보를 추출하고, 평탄화를 하는 영역\n",
        "conv1 = Conv1D(filters = 128,\n",
        "               kernel_size = 3,\n",
        "               padding = 'valid',\n",
        "               activation = tf.nn.relu)(dropout_emb)\n",
        "pool1 = GlobalMaxPool1D()(conv1)\n",
        "\n",
        "conv2 = Conv1D(filters = 128,\n",
        "               kernel_size = 4,\n",
        "               padding = 'valid',\n",
        "               activation = tf.nn.relu)(dropout_emb)\n",
        "pool2 = GlobalMaxPool1D()(conv2)\n",
        "\n",
        "conv3 = Conv1D(filters = 128,\n",
        "               kernel_size = 5,\n",
        "               padding = 'valid',\n",
        "               activation = tf.nn.relu)(dropout_emb)\n",
        "pool3 = GlobalMaxPool1D()(conv3)\n",
        "\n",
        "# 3, 4, 5-gram 이후 합치기\n",
        "concat = concatenate([pool1, pool2, pool3]) #concatenate는 특징맵 결과를 하나로 묶어줌\n",
        "\n",
        "#fully connected layer\n",
        "hidden = Dense(128, activation = tf.nn.relu)(concat)\n",
        "dropout_hidden = Dropout(rate = dropout_prob)(hidden)\n",
        "logits = Dense(3, name='logits')(dropout_hidden) #logits을 점수라 부름 출력 노드에 3개의 점수가 출력됨\n",
        "predictions = Dense(3, activation=tf.nn.softmax)(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sB1H_C-Q2nUr"
      },
      "outputs": [],
      "source": [
        "# 모델 생성 5\n",
        "model = Model(inputs=input_layer, outputs=predictions)\n",
        "\n",
        "model.compile(optimizer ='adam',\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUrLZ_YM4RnF",
        "outputId": "89363405-476e-4375-a848-6c0a16665055"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "414/414 - 14s - loss: 0.8912 - accuracy: 0.5804 - val_loss: 0.5434 - val_accuracy: 0.8046 - 14s/epoch - 33ms/step\n",
            "Epoch 2/5\n",
            "414/414 - 3s - loss: 0.5017 - accuracy: 0.8157 - val_loss: 0.2763 - val_accuracy: 0.9179 - 3s/epoch - 6ms/step\n",
            "Epoch 3/5\n",
            "414/414 - 3s - loss: 0.3150 - accuracy: 0.8910 - val_loss: 0.1689 - val_accuracy: 0.9446 - 3s/epoch - 7ms/step\n",
            "Epoch 4/5\n",
            "414/414 - 4s - loss: 0.1851 - accuracy: 0.9375 - val_loss: 0.1095 - val_accuracy: 0.9674 - 4s/epoch - 10ms/step\n",
            "Epoch 5/5\n",
            "414/414 - 3s - loss: 0.1315 - accuracy: 0.9594 - val_loss: 0.0724 - val_accuracy: 0.9793 - 3s/epoch - 7ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f58c8510760>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#모델 학습하기 6\n",
        "model.fit(train_ds, validation_data =val_ds, epochs=epoch, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVEl2zrxTbbB",
        "outputId": "ac759d6a-0c7c-42c7-b878-9f7ad448289b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60/60 - 0s - loss: 0.0800 - accuracy: 0.9746 - 364ms/epoch - 6ms/step\n",
            "accuracy : 97.461927\n",
            "loss : 0.080039\n"
          ]
        }
      ],
      "source": [
        "# 모델 평가 7 \n",
        "loss, accuracy = model.evaluate(test_ds, verbose=2) # evaluate()함수를 이용해 성능을 평가함 \n",
        "print('accuracy : %f' % (accuracy *100))\n",
        "print('loss : %f' % (loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2r_FJvtJT1v1"
      },
      "outputs": [],
      "source": [
        "model.save('cnn_model.h5') # 모델 저장 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF9BjW-ZUgTH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "virtual",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "e464750dbb2f8340b2e9915e50c1e8c88c6015cc52b93a601b096fd1bd0da1cd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
